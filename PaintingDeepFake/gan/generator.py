from tensorflow.keras import layers
import tensorflow as tf

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

class Generator(object):
    """ 
    The generator of the Generative Adversarial Network. 
    This neural neural network will try to learn to produce images in a such a way that they are indistinguishable from the original pictures.
    The images are generated by starting from an array of random data and passing that through the network.
    """

    def __init__(self, seed_size, output_size):
        """
        Create a new Generator model with the specified input and output size.

        Args:
            seed_size (int): The size of the input array of random data.
            output_size (int): The size of the image. The output image will be of size (output_size, output_size, 3).
        """
        self._model = self._make_model(seed_size, output_size)
        self._seed_size = seed_size
        self._optimizer = tf.keras.optimizers.Adam(1e-4)

    def _make_model(self, input_size, output_size):
        model = tf.keras.Sequential()
        model.add(layers.Dense((output_size // 4) * (output_size // 4) * 256, use_bias=False, input_shape=(input_size,)))
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Reshape((output_size // 4, output_size // 4, 256)))
        assert model.output_shape == (None, output_size // 4, output_size // 4, 256)

        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
        assert model.output_shape == (None, output_size // 4, output_size // 4, 128), f"{(None, output_size // 4, output_size // 4, 128)} != {model.output_shape}"
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
        assert model.output_shape == (None, output_size // 2, output_size // 2, 64)
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
        assert model.output_shape == (None, output_size, output_size, 3)

        return model

    def loss(self, fake_output):
        return cross_entropy(tf.ones_like(fake_output), fake_output)

    def optimizer(self):
        return self._optimizer

    def trainable_variables(self):
        return self._model.trainable_variables

    def model(self):
        return self._model

    def __call__(self, training=False):
        noise = tf.random.normal([1, self._seed_size])
        return self._model(noise, training=training)
        

